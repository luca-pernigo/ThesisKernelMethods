TODO:

    IMPLEMENTATION:
    -Clean NLS test implemntation and put it into src python file

    - MMD(X,Y) X is original data while Y are the generated samples; so idea is that we have mean embeddings (standard,
    Nyström, Piv Cholesky, KernelHerding) we compute their inverse cdf such that we can simulate Y 
    and finally we can compute MMD. remember kernel herding we dont need to invert cdf.
    For MMD we compare xtrain with generated sample and see the performance of our methods.

    LATEX:

    - Explain the importance of standardizing the right way
    explain reasoning for why dividing multiplying the kernel parameter by the standard deviation in the exponent
    is equivalent to calculating the kernel of standardized data.

    - Explain how data have been retrieved, explain it in data and exploratory analysis chapter
    - Explain upon which conditions KME and KDE are the same and when they differ

    - NO (in this paper "Nyström Kernel Mean Embeddings" they use Nyström approximation to reduce computations,
    thus try pivoted cholesky to see how it behaves. People have considered pivoted cholesky for kernels in general
    but have not considered Pivoted Cholesky Kernel Mean Embeddings.)


DONE:
- Make polyfit for every desired order
- python code in src all lowercase for convention
- Consider spline instead of polynomial so that we mitigate oscillations ==> results are reasonable

- Risolvere sistema rational polynomial con QR instead of least squares normal equations formula
- Clean Elaboration data code

- Added Non linear least squares to approximate the inverse of cdf, remeber it is import to fit a good ansatz for a
succesful non linear least squares

1708180016
- Newton's method to find inverse of cdf
- Try by applying cdf inverse method to the smoothed data of kernel mean embedding
- Correct the Kernel mean embedding step