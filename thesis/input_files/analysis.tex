Analysis of experiments and results
\\
Building on the theory introduced in \ref{ch:point} and \ref{ch:prob}, this section covers the experiments carried out and the results obtained.

% - Comments
% \\
% - Comparison
% \\
% - Table of loss scores
% \\
% Plots:
% \\
% - Plots for visualizing timeseries with quantiles bounds
% \\
% - Other plots that will come up to mind

\section{Point forecasting}
This section carries out a comparative study between the state of the art methods for point forecasting, introduced in \ref{ch:point}.
As use case, we will consider the task of predicting the electric load from the GEFCom 2014 dataset.
In such setting we considered the following regressors
\begin{itemize}
    \item Day
    \item Hour
    \item Month
    \item Day of the week
    \item Is holiday
    \item Weather temperature
\end{itemize}
Methods will be compared by means of the RMSE, MAE and MAPE scores, see section \ref{metrics}.
\subsection{Multiple linear regression}
To get started, standard multiple linear regression has been applied, see fig \ref{fig:mlr_price} for a visualisation. 
\begin{figure}
    \includegraphics[width=\textwidth]{images/mlr_price.png}
    \caption{Multiple linear regression prediction}
    \label{fig:mlr_price}
\end{figure}

The resulting RMSE is 30.59.
What can be concluded, is that multiple linear regression is capable of catching the daily seasonality. Nevertheless, it cannot catch the range of the price series properly.

\subsection{Trigonometric seasonality Box-Cox transformation ARMA errors trend and seasonal components}
Next, we tried with the autoregressive approach. Unfortunately, AR, ARIMA and SARIMA models did not perform as expected, their output was slightly better than the one of linear regression. This is probably due to the fact that, the data considered entails two kinds of seasonalities, while ARIMA models can only handle one at a time. We remind the reader, that the electricity load time series involves both daily and weekly seasonalities. Hence, the need for a more advanced time series model. The Tbats \cite{de2011forecasting} model is a forecasting method capable of handling complex patterns in the data. Its name stands for trigonometric seasonality, Box-Cox transformation, ARMA errors, trend and seasonal components. 
Tbats forecast is visualized in figure \ref{fig:tbats_price}, meanwhile its RMSE is 15.08.
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/tbats_price.png}
    \caption{Tbats prediction}
    \label{fig:tbats_price}
\end{figure}
From the plot we see that Tbats is capable of catching the lows and average trend, conversely it has some difficulties handling the price peaks.
\subsection{Prophet}
Following, the prophet model has been considered. 
We get started by considering the base implementation. In such setting, prophet takes in as input the time series object and learns its data generating process.
This method achieves a RMSE of 23.96, its prediction is visualized in figure \ref{fig:prophet_price_1}.
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/prophet_price_1.png}
    \caption{Prophet prediction}
    \label{fig:prophet_price_1}
\end{figure}
What can be seen is that, prophet models correctly the average trend but does not model peaks and lows precisely. 
Next, a more complex model was trained. We added the weather temperature, the square of it and the categorical variable for holidays effect as regressors. Furthermore, we also applied a log transformation to the dependet variable. Doing so, RMSE went down to 10.29. Forecast is visualized in \ref{fig:prophet_price2}, moreover, figure \ref{fig:prophet_price2.1} and figure \ref{fig:prophet_price2.2} break down the trend and the seasonalities respectively.
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/prophet_price2.png}
    \caption{Prophet v2 prediction}
    \label{fig:prophet_price2}
\end{figure}

\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/prophet_price2.1.png}
    \caption{Seasonalities breakdown}
    \label{fig:prophet_price2.1}
\end{figure}

\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/prophet_price2.2.png}
    \caption{Prices trend}
    \label{fig:prophet_price2.2}
\end{figure}
Prophet alongside with the right features is a good model in the context of electricity price forecasting, it is capcable of catching the trend, the peak and the lows. 

\subsection{K-nearest neighbours}
Afterwards, K-nearest neighbours has been considered. In doing so, we cross validated the number of neighbours to get the best model instance. The forecast is visualized in figure \ref{fig:knn_price}, RMSE is 12.54.
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/knn_price.png}
    \caption{K-nearest neighbour regression}
    \label{fig:knn_price}
\end{figure}
What it can be said is that, K-nearest neighbour is capable of predicting prices well by averaging past data.

\subsection{Support vector regression}
Coming up we have support vector regression. In applying this model we used gridsearch crossvalidation to search for the best regularization parameter C. Support vector regression achieves an RMSE of 23.24, for the prediction see figure \ref{fig:svr_price}.
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/svr_price.png}
    \caption{Support vector regression prediction}
    \label{fig:svr_price}
\end{figure}
Visually, we see that the SVR performs similar to multiple linear regression, as expected. Like multiple linear regression, SVR can model the daily seasonality but the point prediction is off in terms of highs and lows.


\subsection{Long short term memory}
We used the torch library in order to code our LSTM regressor.
Various hyperparameters combinations were tried during hyperparameter tuning. The final hyperparameters we set on follows.
\begin{itemize}
    \item hidden\_size= 64
    \item num\_layers= 2
    \item output\_size= 1
    \item num\_epochs= 30
    \item learning\_rate= 0.001
    \item batch\_size= 32
    \item window\_size= 24
\end{itemize}
LSTM forecast is reported in table \ref{fig:lstm_price}, it achieves a RMSE of 9.1782
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/lstm_price.png}
    \caption{Long short term memory prediction}
    \label{fig:lstm_price}
\end{figure}

\subsection{Kernel ridge regression}
Subsequently, kernel ridge regression was considered. The kernel considered is the radial basis gaussian function.
Cross validation was carried jointly over the rbf kernel bandwith and the regularization constant.
The RMSE achieved is 9.86, figure \ref{fig:krnridge_price} reports the prediction.
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/krnridge_price.png}
    \caption{Kernel ridge prediction}
    \label{fig:krnridge_price}
\end{figure}
We can observe that, kernel ridge regression models accurately the electricity time series.

\subsection{Kernel support vector regression}
The last model we considered in this setting was the kernel support vector regression.
RMSE achieved with this method is 9.10, forecast is reported in figure \ref{fig:krnsvr_price}.

\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/krnsvr_price.png}
    \caption{Kernel support vector prediction}
    \label{fig:krnsvr_price}
\end{figure}
We can observe that kernel support vector regression is one of the best performing techniques between the one considered.

\subsection{Results}
This section reports the table comparing the considered methods scores 
% in the context of electricity load forecasting.

% - table 
% |methods|tasks RMSE|
% |       |X          |       
% |       |X          |    
Table \ref{tab:point_RMSE} reporst the RMSE scores for each of the considered methods. It can be seen that kernel based methods are the ones achieving the lowest RMSE; specifically, we have KrnSVR, KrnRidge and KNN being among the top methods on the majority of tasks.

\begin{table}[!ht]
    \caption{Root mean squared errors}
    \label{tab:point_RMSE}
    \begin{adjustbox}{width=\textwidth, height=3cm}
        \begin{tabular}{lrrrrrrrrrrrrrrr}
            \toprule
            Method/RMSE & Task 1 & Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Task 7 & Task 8 & Task 9 & Task 10 & Task 11 & Task 12 & Task 13 & Task 14 & Task 15 \\
            \midrule
            MLR & 30.5892 & 29.0347 & 75.1682 & 63.1376 & 42.1156 & 36.6233 & 39.2496 & 38.8286 & 47.1307 & 68.0494 & 61.6005 & 30.3751 & 34.5523 & 33.4287 & 33.5581 \\
            TBATS & 15.0886 & 31.9720 & 99.1260 & 84.4890 & 51.9740 & 34.9055 & 18.4445 & 38.3246 & 74.1117 & 98.6600 & 84.3050 & 38.8433 & 16.6080 & 28.9902 & 41.6194 \\
            Prophet & 10.2936 & 14.4358 & 38.7551 & 63.4787 & 19.7474 & 17.5065 & 12.6926 & 14.2665 & 17.5466 & 23.5944 & 43.6666 & 20.8637 & 16.9493 & 19.1626 & 23.3889 \\
            KNN & 12.5429 & 11.6699 & 24.5057 & 18.3310 & 13.1821 & \textbf{11.9238} & 12.0044 & 14.5165 & 16.3132 & 15.1831 & 37.6457 & 16.4690 & 12.0324 & 11.3102 & 14.1717 \\
            SVR & 23.2421 & 25.8525 & 78.8043 & 67.6209 & 42.3748 & 32.7845 & 30.5971 & 35.3660 & 55.4213 & 77.9660 & 68.5799 & 30.2451 & 27.0345 & 28.3652 & 32.1480 \\
            LSTM & 9.1782 & 12.0669 & \textbf{22.7048} & \textbf{16.2087} & 16.0964 & 12.7936 & \textbf{10.8559} & 14.6173 & 19.7303 & 18.0200 & 43.2051 & 17.1856 & 10.3106 & 12.1347 & 17.5849 \\
            KrnRidge & 9.8619 & 11.6101 & 37.7824 & 35.2459 & \textbf{12.5160} & 14.5911 & 12.8791 & 17.4385 & 16.1131 & 17.1938 & 37.6961 & 14.0076 & 9.8441 & 10.7491 & 13.2975 \\
            KrnSVR & \textbf{9.1028} & \textbf{11.3117} & 22.9281 & 19.2132 & 12.6331 & 12.6018 & 11.3537 & \textbf{12.9506} & \textbf{14.9731} & \textbf{11.7765} & \textbf{37.3797} & \textbf{13.2694} & 
            \textbf{8.8522} & \textbf{10.6185} & \textbf{13.2602} \\
            \bottomrule
            \end{tabular}            
    \end{adjustbox}
\end{table}


The MAE scores are contained in table \ref{tab:point_MAE}. Similarly to above, we can conclude that kernel methods stand out for achieving also the lowest MAE score.
\begin{table}[!ht]
    \caption{Mean absolute errors}
    \label{tab:point_MAE}
    \begin{adjustbox}{width=\textwidth, height=3cm}
        \begin{tabular}{lrrrrrrrrrrrrrrr}
            \toprule
             Method/MAE & Task 1 & Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Task 7 & Task 8 & Task 9 & Task 10 & Task 11 & Task 12 & Task 13 & Task 14 & Task 15 \\
            \midrule
            MLR & 27.7219 & 24.3823 & 65.4016 & 52.3873 & 34.8465 & 31.2663 & 35.9420 & 34.7316 & 37.0273 & 54.7075 & 48.9327 & 26.1975 & 31.8720 & 29.2306 & 28.3428 \\
            TBATS & 10.5408 & 23.8417 & 91.3153 & 74.5734 & 40.3258 & 26.0092 & 14.2141 & 24.7818 & 62.0452 & 86.7125 & 72.6912 & 28.4235 & 11.1184 & 21.1038 & 31.8555 \\
            Prophet & 8.6139 & 11.4307 & 28.3509 & 46.4480 & 15.6290 & 14.1278 & 10.1574 & 11.2360 & 14.0072 & 18.9565 & 27.1142 & 16.9678 & 14.6615 & 16.1750 & 18.3043 \\
            KNN & 9.4003 & 9.1126 & 19.6213 & 14.4906 & 10.6461 & \textbf{9.4354} & 8.7441 & 10.0065 & 12.4714 & 11.2006 & 20.3087 & 12.5416 & 9.2558 & 8.5937 & 10.5451 \\
            SVR & 20.2443 & 21.2526 & 69.3704 & 57.1107 & 34.3651 & 27.5875 & 27.4651 & 29.4204 & 43.7009 & 64.3522 & 55.4307 & 25.1080 & 24.0294 & 24.4049 & 26.5869 \\
            LSTM & 7.5217 & 9.4767 & \textbf{17.9851} & \textbf{13.2158} & 11.8118 & 9.8607 & \textbf{8.0561} & 10.8480 & 15.1008 & 13.8108 & 25.9906 & 13.3588 & 8.1526 & 9.8000 & 14.3186 \\
            KrnRidge & 7.5965 & 9.2130 & 28.2484 & 24.2366 & \textbf{10.0589} & 10.7758 & 9.5788 & 11.2166 & 12.3427 & 12.3306 & 20.0213 & 10.8434 & 7.1625 & \textbf{7.9710} & 10.0767 \\
            KrnSVR & \textbf{6.9581} & \textbf{8.6989} & 18.7739 & 15.1398 & 10.1395 & 9.5927 & 8.5731 & \textbf{9.0033} & \textbf{11.0618} & \textbf{8.5486} & \textbf{19.7383} & \textbf{10.5545} & \textbf{7.0395} & 8.1926 & \textbf{9.8125} \\
            \bottomrule
            \end{tabular}            
    \end{adjustbox}            
\end{table}

Finally table \ref{tab:point_MAPE} reports the MAPE score for each method. What can be concluded is that kernel regressors are also the best in terms of MAPE score; with KrnSVR being the top one.
\begin{table}[!ht]
    \caption{Mean absolute percentage errors}
    \label{tab:point_MAPE}
    \begin{adjustbox}{width=\textwidth, height=3cm}
        \begin{tabular}{lrrrrrrrrrrrrrrr}
            \toprule
             Method/MAPE & Task 1 & Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Task 7 & Task 8 & Task 9 & Task 10 & Task 11 & Task 12 & Task 13 & Task 14 & Task 15 \\
            \midrule
            MLR & 0.2470 & 0.1935 & 0.3030 & 0.2601 & 0.2474 & 0.2667 & 0.3498 & 0.3058 & 0.1954 & 0.2432 & 0.4234 & 0.2114 & 0.2926 & 0.2502 & 0.2145 \\
            TBATS & 0.0833 & 0.1650 & 0.4285 & 0.3731 & 0.2482 & 0.1849 & 0.1234 & 0.1634 & 0.3181 & 0.4021 & 0.4880 & 0.1761 & 0.0905 & 0.1555 & 0.2058 \\
            Prophet & 0.0737 & 0.0854 & 0.1324 & 0.2453 & 0.1134 & 0.1169 & 0.0938 & 0.0912 & 0.0880 & 0.0992 & 0.3757 & 0.1389 & 0.1343 & 0.1356 & 0.1370 \\
            KNN & 0.0778 & 0.0682 & 0.0924 & 0.0761 & 0.0751 & 0.0740 & 0.0784 & 0.0744 & 0.0725 & 0.0555 & \textbf{0.3115} & 0.0888 & 0.0816 & 0.0699 & 0.0765 \\
            SVR & 0.1808 & 0.1640 & 0.3211 & 0.2829 & 0.2317 & 0.2260 & 0.2676 & 0.2451 & 0.2215 & 0.2867 & 0.4354 & 0.1870 & 0.2216 & 0.2044 & 0.1933 \\
            LSTM & 0.0642 & 0.0706 & 0.0896 & \textbf{0.0722} & 0.0798 & 0.0772 & \textbf{0.0710} & 0.0854 & 0.0909 & 0.0741 & 0.3578 & 0.0976 & 0.0719 & 0.0778 & 0.1037 \\
            KrnRidge & 0.0625 & 0.0685 & 0.1281 & 0.1207 & 0.0718 & 0.0797 & 0.0812 & 0.0760 & 0.0696 & 0.0587 & 0.3127 & 0.0779 & \textbf{0.0591} & \textbf{0.0637} & 0.0722 \\
            KrnSVR & \textbf{0.0568} & \textbf{0.0642} & \textbf{0.0892} & 0.0788 & \textbf{0.0714} & \textbf{0.0724} & 0.0740 & \textbf{0.0665} & \textbf{0.0629} & \textbf{0.0428} & 0.3122 & \textbf{0.0781} & 0.0607 & 0.0646 & \textbf{0.0713} \\
            \bottomrule
            \end{tabular}            
    \end{adjustbox}
\end{table}

\section{Probabilistic forecasting}
In this subsection we compare performance of quantile regressors.
\\
- experiment for quantile estimator on gefcom2014 and we are good


\section{Conclusions}