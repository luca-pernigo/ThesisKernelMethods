This section is intended to explain and aid for reproducibility studies. Hereafter, the specific libraries used and the custom implementations are thoroughly documented.

For the list of Python packages needed, see the requirement.txt file on the \href{https://github.com/luca-pernigo/ThesisKernelMethods}{github repo}.
% - indicate computer specifics
All experiments have been carried out on a 3.2 GHz 16GB Apple M1 Pro.


% Section documenting code

% - Explain how methods' implementation has been
% adapted to my specific setting.

% - Explain in detail how to my src code has been implemented
% its rationale and how to use it.

% - As I explain code scripts go over the test, to 
% explain better my ideas.

% - Indicate also hyperparameters maybe in each subsection

\section{Point forecasting}
\subsection{Multiple linear regression}
For multiple linear regression the one from the \href{https://scikit-learn.org/stable/}{sklearn} library has been used.

\subsection{Trigonometric seasonality Box-Cox transformation AR\-MA errors trend and seasonal components}
The Tbats implementation is available at \href{https://github.com/intive-DataScience/tbats}{https://github.com/intive-Data-Science/tbatsx}.
In our application we specified as hyperparameters the length of seasons, that is, 24 for the daily seasonality and 168 for the weekly seasonality.

\subsection{Prophet}
The \href{https://facebook.github.io/prophet/docs/quick_start.html}{prophet} model has been applied by employing the Python API provided by Meta.

\subsection{K-nearest neighbours}
The object KNeighborsRegressor of the \href{https://scikit-learn.org/stable/}{sklearn} module neighbors has been used with 12 neighbours and the Euclidean distance.

\subsection{Support vector regression}
The object SVR of the \href{https://scikit-learn.org/stable/}{sklearn} module svm has been used by specifying the linear kernel.

\subsection{Long short term memory}
The LSTM predictor has been built using the \href{https://pytorch.org}{torch} library, see Section \ref{sec:lstm point} for architecture details.

\subsection{Kernel ridge regression}
The object KernelRidge of the \href{https://scikit-learn.org/stable/}{sklearn} module kernel\_ridge has been used with the Gaussian RBF kernel.

\subsection{Kernel support vector regression}
The object SVR of the \href{https://scikit-learn.org/stable/}{sklearn} module svm has been used with the Gaussian RBF kernel.
% by specifying rbf as the kernel parameter.

\section{Probabilistic forecasting}
\subsection{Linear quantile regression}
The implementation of \href{https://www.statsmodels.org/dev/generated/statsmodels.regression.quantile_regression.QuantReg.html}{quantile\_regression.QuantReg} from the regression module of statsmodels has been used.
The model is fitted through iterative reweighted least squares.
\subsection{Quantile gradient boosting machine}
The implementation of \href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html}{GradientBoostingRegressor} from the sklearn.ensemble submodule has been used.
\subsection{Quantile forest}
The implementation of \href{https://pypi.org/project/quantile-forest/}{quantile\_forest.RandomForestQuantileRegressor} has been used. This estimator is compatible with scikit-learn API \cite{Johnson2024}.
\subsection{Kernel quantile regression}
Kernel quantile regression had no previously implemented Python open source library, thus, the need of implementing our own version.
\\
The scikit-learn team provides a project template for the creation of estimators compatible with scikit-learn functionalities. Therefore, the KQR class is derived from the scikit-learn BaseEstimator and the mixin class RegressorMixin.
Our KQR class is initialised by providing a quantile, the regularisation term $C$, the kernel family and its corresponding hyperparameters.
\\
In the fit method, we set up and solve the convex optimisation problem through the interior point algorithm. This algorithm is taken from the cvxopt library, see its official manual \citeW{vandenberghe2010cvxopt} for a reference.
When using this library, it is important to keep two things in mind. First this library assumes the quadratic term of the optimisation problem to be multiplied by the 0.5 factor, thus, we just have to provide the $Q$ matrix with no 0.5 in front.
Secondly, in order to specify multiple inequalities we have to stack them and provide them as a single matrix.
\\
Once a solution to the convex problem has been found, we create a mask for the support vectors of the estimator in order to estimate the constant term of our kernel quantile regressor.
\\
In the predict method, we pass a matrix $X\_eval$ of independent variables, next we compute the kernel matrix between $X\_train$ and $X\_eval$ and obtain $y\_eval$ with the formula $y=\alpha^\intercal K+b$.
\\
This estimator is compatible with in built scikit-learn methods like gridsearch, crossvalidation and scoring rules. Moreover, this code is compatible with sklearn in built kernel functions \href{https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise}{sklearn.metrics.pairwise} and \href{https://scikit-learn.org/stable/modules/classes.html#module-sklearn.gaussian_process.kernels}{sklearn.gaussian\_process.kernels}. Functions supported by our kernel quantile regression include: Gaussian RBF, Absolute Laplacian, Matern 3/2, Matern 5/2, Linear, Cosine, Sigmoid, Periodic, Polynomial and custom composition of kernels. 
Kernels from these two submodules use a different convention in the hyperparameters. The former has the lengthscale parameter in the numerator while the latter in the denominator. In order to meaningfully compare across different kernels we made them consistent in this aspect. For this purpose we adhere to the convention of having lengthscale hyperparameters in the denominator.

% NOTICE, these kernel implementations differ in the form of passed parameters in the former we pass in the numerator gamma while in the latter we pass l, which stands for length scale in the denominator. To cross validate overt the same range of parameter across the different kernels we made sura that gamma=1/l.


% \\
% Up to now, there are only two open source implementation of the quantile kernel regression. Nevertheless they are both in R, that is there exists no python, matlab or julia open source implementation. 

% Following are reported the results of a comparative study between our own implementation and the one of the R library kernlab


% (i guess it is in C or C++ and then binded to R).
% \subsubsection{Python versus R implementation}
% In this section, a comparison study has been carried out in order to inspect the competitiveness of our implementation with the existing one for the R programming language.