When it comes to probabilistic forecasting, there are a couple of standard practical approaches; conceptually they can be grouped into two main categories. The former class of approaches tries modelling the distribution of the observed data directly. The latter family of approaches constructs first a point forecast and then learns the distribution of the model's errors.
\section{Quantile regression}
\input{input_files/qr.tex}

\section{Kernel density estimation}
\input{input_files/kde.tex}

\section{Ensemble methods}
\input{input_files/ensemble.tex}

% \section{Copula models}
\section{Quantile forest}
Meinshausen \cite{meinshausen2006quantile} extends the idea of random forest \cite{breiman2001random} generalising it, the result is the quantile forest algorithm. Quantile forest allows us to estimate conditional quantiles in a non parametric fashion.
\\
In order to understand this algorithm it is necessary to first cover the theory of decision trees and random forests.
- decision trees
- random forest
- quantile forest
\section{Quantile gradient boosting machine}


\section{DMLP}
%gluon nn
% train a beta distribution to learn its parameters
\section{DeepAR}

\section{Kernel methods}
\subsection{Kernel quantile regression}
The idea of quantile regression has been extended to kernel methods by Takeuchi et al. \cite{takeuchi2006nonparametric}.
There, they minimize a risk plus regularizer defined as follows.
\begin{equation}\label{eq:kqr_min1}
    R[f]:=\frac{1}{m}\sum\limits_{i=1}^{m}\rho_q(y_i-f(x_i))+\frac{\lambda}{2}\|g\|_H^2
\end{equation}
where $f=g+b$, $g \in H$ and $b \in R$.
Using the link between RKHS and feature spaces, we can rewrite $f(x)=\langle w, \phi(x) \rangle+b$. Moreover, note that the RKHS norm is defined as follows $\|f\|_{H}=\inf\{\|w\|_{F}:w\in F,f(x)=\langle w,\varphi (x)\rangle _{F},\forall x\in X\}$, where $F$ is the feature space.
 Doing so we obtain a minimization problem equivalent to minimizing equation \ref{eq:kqr_min1}.
\begin{equation}\label{eq:kqr_min2}
    \begin{aligned}
    \min_{w,b} \quad & C \sum \limits_{i=1}^{m}
    q(y_i-\langle w, \phi(x) \rangle-b)+ (1-q)(-y_i+\langle w, \phi(x) \rangle+b)+ \frac{1}{2}\|w\|^2\\
    \end{aligned}
    \end{equation}
Note the division by $\lambda$ so that $C=\frac{1}{\lambda m}$.
\\
We can next rephrase the optimisation in \ref{eq:kqr_min2} by introducing the slack variables $\xi_i$ and $\xi_i^*$.
\begin{equation}\label{eq:kqr_min3}
    \begin{aligned}
        \min_{w,b,\xi_i,\xi_i^*} \quad & C \sum \limits_{i=1}^{m}
        q \xi_i+ (1-q)\xi_i^*+ \frac{1}{2}\|w\|^2\\
    \textrm{s.t.} \quad & y_i-\langle w, \phi(x) \rangle-b \leq \xi_i\\
    & -y_i+\langle w, \phi(x) \rangle+b \leq \xi_i^*\\
      &\xi_i\geq0    \\
      &\xi_i^*\geq0    \\
    \end{aligned}
    \end{equation}
In order to make it more compact, we rewrite equation \ref{eq:kqr_min3} in matrix notation.
\begin{equation}\label{eq:kqr_min4}
    \begin{aligned}
        \min_{w,b\xi,\xi^*} \quad & C q \xi^\intercal \mathbb{1}+ C q (\xi^*)^\intercal \mathbb{1}+ \frac{1}{2}w^\intercal w\\
    \textrm{s.t.} \quad & y-\Phi^\intercal w -b \preceq \xi\\
    & -y+\Phi^\intercal w +b \preceq \xi^*\\
      &\xi\succeq0    \\
      &\xi^*\succeq0    \\
    \end{aligned}
    \end{equation}
Consider now, the lagrangian $L$ associated to \ref{eq:kqr_min4}
\begin{equation}\label{eq:kqr_min5}
    \begin{aligned}
    L(w,b,\xi,\xi^*)= & C q \xi^\intercal \mathbb{1}+ C q (\xi^*)^\intercal \mathbb{1}+ \frac{1}{2}w^\intercal w- \alpha^\intercal(\xi - y+\Phi^\intercal w +b)
    - (\alpha^*)^\intercal(\xi^* +y-\Phi^\intercal w -b)
    \\
    & -\nu^\intercal \xi - (\nu^*)^\intercal \xi^*
\end{aligned}
\end{equation}
The next step is deriving its dual formulation, since it is easier and more efficient to solve. This because the dual problem has the useful property of being always convex.

\begin{definition}
    The dual function associated to the lagrangia $L(x,\lambda)$ is given by $g(v)=\inf_x L(x,\lambda)$
\end{definition}
where $\lambda$ is called the lagrange multiplier of the optimization problem. Such dual formulation has an useful property, that is \begin{equation}\label{weak_duality}
    g(\lambda)\leq p^*
\end{equation}
where $p^*$ is the optimal value of your optimization problem.
Consider a simple lagrangian $L(x,\lambda)=f(x)+\sum \lambda_i r_i(x) +\sum v_i h_i(x)$, where $r_i(x)$ are inequality constraints while $h_i(x)$ are equality constraints of the problem. Then it can be noted that the lower bound on $p^*$ is non trivial only when the lagrange multiplier $\lambda \succeq 0$. Therefore, the idea is that by maximizing the dual function subject to the contraint $\lambda \succeq 0$, we can obtain an approximate or perfect solution to the primal problem.
\\
To explain why we may or not be able to attain the best solution to the primal problem by maximizing its dual we have to introduce the concept of duality.
\\
We use $d^*$ to denote the optimal value of the lagrange dual problem; we can think of it as the best lower bound on $p^*$. 
\\
The inequality \ref{weak_duality} is called weak duality. The difference $p^*-d^*$ is said the optimal duality gap; it is the gap between the optimal value of the primal problem and the best lower bound on it that can be obtained from the Lagrange dual function. Moreover, note that the optimal duality gap is always nonnegative.
\\
We say that strong duality holds, when the optimal duality gap is zero; in other words, the lagrange dual bound is tight.
\\
Constraint qualifications are condtions under which strong duality holds; one of the most popular is Slater's condition.
\begin{equation}\label{slater_condition}
    \begin{aligned}
        \exists x \in \textrm{relint} \ D \ \textrm{s.t.} & \ r_i(x)<0, \quad i=1, \dots, m \\
        & h_i(x)=b
    \end{aligned}
\end{equation}
Where relint $D$ is the relative interior of $ D:=\cap _{i=0}^{m}\operatorname {dom} (r_{i})$
\\
Slater's theorem naturally follows.
\begin{theorem}
    If Slater's condtion holds and the problem is convex then strong duality holds.
\end{theorem}
We can now check that our optimization problem posseses strong duality by checking Slater's condition.
\\
In our case we don't have any equality constraint, so we do not have to worry about the $h_i(x)=0$ term in \ref{slater_condition}. All we have to check is the convexity of our problem and that there exist a $x$ such that $r_i(x)<0$.
For convexity, a sufficient condition is the positive definiteness of $Q$ in the quadratic programming problem 
\begin{equation}
    \begin{aligned}
        \min \quad & x^\intercal Q x+ c^\intercal x \\        
        s.t \quad& Ax\preceq b
    \end{aligned}
\end{equation}
In our case \ref{eq:kqr_min5}, we have $w^\intercal w$, thus $Q$ is just the identity matrix which satisfies the positive definiteness requirement. Therefore, our problem is convex.
Next we check that Slater's condition holds. Considering first the two non negative constraints on $\xi$ and $\xi^*$, we conclude that $\xi$  and $\xi^*$ have to be greater or equal to zero for the existence of an $x$ satisfying Slater's condition. Thus, let us suppose that $0 \leq \xi \leq \alpha$ and $0 \leq \xi^* \leq \alpha$.
\\
Next, let us consider the other two inequalities and make the following ansatz.
\\
\begin{equation}
    \begin{aligned}
        w=& \Phi^\intercal(\Phi \Phi^\intercal)^{-1} y\\
        b<& \alpha
    \end{aligned}
\end{equation}
We then have
\begin{equation}
    \begin{aligned}
        -\xi + y -\Phi\Phi^\intercal(\Phi \Phi^\intercal)^{-1}y-b<&0
        \\
        -\xi^* - y +\Phi\Phi^\intercal(\Phi \Phi^\intercal)^{-1}y+b<&0
    \end{aligned}
\end{equation}
Hence, we conclude that our problem satisfies Slater's condition. Therefore the solution of the dual and primal problem are equivalent.
\\
We end this section with the derivation of the dual problem; that is the convex problem we will solve in order to get the qunatiles prdeiction of our quantile kernel algorithm.
\\
First, derive the dual function of \ref{eq:kqr_min5}.
\begin{equation}
    \begin{aligned}
        g(\alpha, \alpha^*, \nu, \nu^*)=& \inf_x L(x,\lambda)\\
    = & \inf_{\xi, \xi^*, w, b} L(w,b,\xi,\xi^*)
\end{aligned}
\end{equation}
Setting its derivates to zero
\begin{equation}\label{eq:lagrange_derivatives}
    \begin{cases}
        \frac{\partial L}{\partial w}=0 \implies w=\Phi^\intercal(\alpha-\alpha^*)
        \\
        \frac{\partial L}{\partial b}=0 \implies \alpha-\alpha^*=0
        \\
        \frac{\partial L}{\partial \xi}=0 \implies Cq \mathbb{1}-\alpha- \nu=0
        \\
        \frac{\partial L}{\partial \xi^*}=0 \implies C(1-q)\mathbb{1} -\alpha^* -\nu^*=0
    \end{cases}
\end{equation}
As pointed out previously, the lower bound resulting from the dual formulation is non trivial only when the lagrange multipliers are $\succeq 0$. Looking at the last two equations of the system \ref{eq:lagrange_derivatives}, this implies the following two constraints $\alpha \in [0, Cq\mathbb{1}]$ and $\alpha \in [0, C(1-q)\mathbb{1}]$.
\\
Substitute the conditions for an optimum into \ref{eq:kqr_min5} we obtain the dual formulation.
\begin{equation}
    \begin{aligned}
        g(\alpha, \alpha^*)=& \xi^\intercal(Cq\mathbb{1}-\alpha -\nu)+(\xi^*)^\intercal(C(1-q)\mathbb{1}-\alpha^*-\nu^*)-(\alpha-\alpha^*)^\intercal \Phi\Phi^\intercal(\alpha-\alpha^*)
        \\
        & +(\alpha-\alpha^*)^\intercal y-(\alpha-\alpha^*)^\intercal b+\frac{1}{2}(\alpha-\alpha^*)^\intercal \Phi\Phi^\intercal(\alpha-\alpha^*)
        \\
        \\
        g(\alpha, \alpha^*)=& 0+0-\frac{1}{2}(\alpha-\alpha^*)^\intercal \Phi\Phi^\intercal(\alpha-\alpha^*)+(\alpha-\alpha^*)^\intercal y-0
        \\
        g(\alpha, \alpha^*)=& -\frac{1}{2}(\alpha-\alpha^*)^\intercal \Phi\Phi^\intercal(\alpha-\alpha^*)+(\alpha-\alpha^*)^\intercal y
    \end{aligned}
\end{equation}
Defining $\alpha=(\alpha-\alpha^*)$ and letting $K$ the kernel matrix, we have that the dual optimisation problem reads as follows
\begin{equation}\label{eq:kqr_min6}
    \begin{aligned}
        \max_{\alpha} \quad & -\frac{1}{2}\alpha^\intercal K\alpha+\alpha^\intercal y\\
    \textrm{s.t.} \quad & 
    C(q-1)\mathbb{1}\preceq \alpha \preceq Cq\mathbb{1}\\
    &\alpha^\intercal\mathbb{1}=1
    \end{aligned}
    \end{equation}
Switching sign, we rephrase it as a minisation problem, which is the common practice in convex optimisation.
\begin{equation}\label{eq:kqr_min7}
    \begin{aligned}
        \min_{\alpha} \quad & +\frac{1}{2}\alpha^\intercal K\alpha-\alpha^\intercal y\\
    \textrm{s.t.} \quad & 
    C(q-1)\mathbb{1}\preceq \alpha \preceq Cq\mathbb{1}\\
    &\alpha^\intercal\mathbb{1}=1
    \end{aligned}
    \end{equation}
The kernel quantile regression estimator is then given by
\begin{equation}
    f(x)=\sum\limits_i \alpha_i k(x_i, x)+b
\end{equation}
Where an unbiased estimator for $b$ is given by
\begin{equation}
    \begin{aligned}
    b=&\frac{1}{I_q}\sum\limits_{i\in I_q} b_i, \\ 
    \textrm{with} \ b_i=& y_i-\sum\limits_k \alpha_k k(x_k, x_i) \\
     \textrm{and} \ I_q=& \{i=1,\dots,n|0<\alpha_i<Cq, 0<\alpha_i<C(1-q)\}
    \end{aligned}
\end{equation}
%contribution is implementation kernel quantile regression for pythos users since there exists only r version
\subsection{Kernel herding}
Select the best point forecasting method and create a probabilistic forecast by modelling its model errors with kernel herding (do something like the residual bootstrap ensembles if it is meaningful and possible to implement).