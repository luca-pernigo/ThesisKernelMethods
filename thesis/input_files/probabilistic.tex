We saw in the previous chapter how point forecasting relates to predicting the expected value of the target variable.
Conversely, in probabilistic forecasting we are interested in modelling a probability distribution over the target variable.
This approach enables us to account for prediction uncertainty and to make more informed decisions.
\section{Quantile regression}
\input{input_files/qr.tex}

\section{Quantile forest}
Meinshausen \cite{meinshausen2006quantile} extends the idea of random forest \cite{breiman2001random} generalising it, the result is the quantile forest algorithm. Quantile forest allows us to estimate conditional quantiles in a non parametric fashion.
\\
In order to understand this algorithm, it is first necessary to cover the theory of decision trees and random forests.
\subsection{Decision trees}
Decision tree methods partition recursively the feature space in a set of binary rectangles and then fit a simple model in each of those partitions (the most straightforward is fitting just a constant).
To get started, we first split the space into two disjoint regions, then we model the response variable by the mean of the observed predicted variables with associated features falling in that specific region. Our goal is selecting the best features and best split point to achieve the best generalizing fit.
For a visualisation consider the Figures \ref{fig:elements_statistical_learning1} and \ref{fig:elements_statistical_learning2}, where the decision tree algorithm is visualised for a regression problem with two independent variables $X_1$ and $X_2$.
\begin{figure}
    \includegraphics[width=0.5\textwidth]{images/elsii1.png}
    \caption{Two-dimensional feature space partitioned by recursive binary splitting \cite{hastie2009elements}}
    \label{fig:elements_statistical_learning1}
  \end{figure}
\\
\begin{figure}
\includegraphics[width=1\textwidth]{images/elsii2.png}
\caption{Partition tree and regression model \cite{hastie2009elements}}
\label{fig:elements_statistical_learning2}
\end{figure}
\\
Suppose now to partition the feature space into M regions $R_1,\dots,R_M$, then the model reads as follows.
\begin{equation}
    f(x)=\sum\limits_{m=1}^{M}c_m\mathbb{I}_{\{x \in R_m\}}
\end{equation}
\\
It follows that the function minimising the sum of squares is the one with $\hat{c}_m=mean(y_i|x_i \in R_m)$.
Finding the best split in terms of minimum sum of squares is computationally infeasible in practice. Therefore we approximate a solution by approaching the problem in a greedy fashion.
Let $j$ denote the splitting variable and $s$ be the split point we define the two half planes
\begin{equation}
    R_1(j,s)=\{X|X_j\leq s\} \quad R_2(j,s)=\{X|X_j >s\}
\end{equation}
Then we search for the $s$ and $j$ that solve
\begin{equation}
    \min_{j,s}\left[\min_{c_1} \sum\limits_{x_i \in R_1(j,s)}(y_i-c_1)^2+\min_{c_2} \sum\limits_{x_i \in R_2(j,s)}(y_i-c_2)^2\right]
\end{equation}
The inner problem is easy, as already pointed out, we will have 
\begin{equation}
    \hat{c}_1=mean(y_i|x_i \in R_1) \quad \hat{c}_2=mean(y_i|x_i \in R_2)
\end{equation}
For the outer problem, we scan through all the $(j,s)$ tuples and pick the best pair. 
Next, one or both of these regions from the previous step are split into two more regions. We recurse this process until some stopping condition is triggered (max number of branches, max depth of tree, threshold on the mean squared error (MSE), minimum number of observations in each leaf node).
Notice, this being a greedy algorithm implies that our final solution is guaranteed to be just a local optimum not a global one.
Even though their simplicity, these models have proved themselves to be really powerful. See Figure \ref{fig:decision_tree} for an example where decision trees with different hyperparameters are fitted to a sine wave plus noise. The most popular among these models is the CART \cite{breiman2017classification} tree, its name comes from the fact that it can handle both classification and regression problems.
\begin{figure}
    \includegraphics[width=\textwidth]{images/decision_tree.png}
    \caption{Decision tree regression}
    \label{fig:decision_tree}
    % image generated on coolab to be quick
\end{figure}
    
\subsection{Bootstrap}
The idea behind bootstrapping is to randomly sample from the training set with replacement $B$ times and then fitting $B$ models to each of the "artificial" datasets. Bootstrapping can serve different tasks. We can use it to assess the accuracy of a parameter %estimate
or of a prediction but also to improve their estimates.

\subsection{Bagging}
Bagging stands for bootstrap aggregation. 
The bagging estimate is defined by $\mathbb{E}_P[\hat{f}^*]$ where $P$ is the empirical distribution putting equal probability on each data point of the training set.
Basically, for each bootstrap fitted model $\hat{f}^{*b}(x)$, we compute the bagging estimate by
\begin{equation}
    \hat{f}_{bag}(x)=\frac{1}{B}\sum\limits_{b=1}^{B}\hat{f}^{*b}(x)
\end{equation}
Bagging is particularly useful in reducing the variance of decision trees, resulting in an improved prediction (bias-variance tradeoff).
Note, this improvement comes from the fact that averaging reduces variance and leaves biases unchanged.

\subsection{Random forest}
Decision trees are characterised by high variance and low bias, thus, they can benefit extremely from bagging. Furthermore, every decision tree generated through bagging will be identically distributed (i.d.), thus the expectation of an average of $B$ trees is probabilistically equivalent to the expectation of any such tree. As a consequence, the bias will stay fixed since the bias of the bagged estimator is the same as that of each individual tree. 
\\
Consider positively correlated i.d.\ random variables, then the variance of their average is 
\begin{equation}
    \rho \sigma^2+\frac{1-\rho}{B}\sigma^2
\end{equation}
The second term disappears as $B$ increases, while the first term depends heavily on the correlation between bagged trees. Random forest consists in reducing the correlation between the trees by randomly selecting $m$ of the $p$ features as candidates for splitting; $m$ tipically takes a value in the order of $\sqrt{p}$ or even 1 with default value $m=\left\lfloor \frac{p}{3} \right\rfloor$, while a good minimum node size is around five. 
\\
Letting $T(x;\Theta_b)$ be the $b^{th}$ bagged tree where $\Theta_b$ denotes the randomness characterising its splits, cutpoints and terminal node values, we have that the random forest regressor is given by
\begin{equation}
    \hat{f}_{rf}^{B}=\frac{1}{B}\sum\limits_{b=1}^{B}T(x;\Theta_b)
\end{equation}
For a simple visualisation compare Figure \ref{fig:random_forest} with Figure \ref{fig:decision_tree}.
\begin{figure}
    \includegraphics[width=\textwidth]{images/random_forest.png}
    \caption{Random forest regression}
    \label{fig:random_forest}
    % image generated on coolab to be quick
\end{figure}


\subsection{Quantile forest}
The key observation here is noting that random forest approximates the conditional expected value $\mathbb{E}(Y|X=x)$ by a weighted average over the observed $y$.
% expected value is the weighted averaged of realizations with weights their respective probabilities.
Hence, we can extend this idea to the full conditional distribution by
\begin{equation}
    F(y|X=x)=P(Y\leq y|X=x)=\mathbb{E}(\mathbb{I}_{\{Y\leq y\}}|X=x)
\end{equation}
All we have to do is approximating $\mathbb{E}(\mathbb{I}_{\{Y\leq y\}}|X=x)$ by a weighted mean over the random variable $\mathbb{I}_{\{Y\leq y\}}$
\begin{equation}
    \hat{F}(y|X=x)=\sum\limits_{i=1}^{n}\omega_i(x)\mathbb{I}_{\{Y_i\leq y\}}
\end{equation}
By swapping $F(y|X=x)$ with $\hat{F}(y|X=x)$ in the definition of conditional quantiles we obtain their respective random forest estimator
\begin{equation}
    \hat{Q}_q=\inf\{y:\hat{F}(y|X=x)\geq q\}
\end{equation}


\section{Quantile gradient boosting machine}
\subsection{Boosting}
With boosting we fit an additive expansion of elementary basis functions; with $M$ basis functions we have 
\begin{equation}
    f(x)=\sum\limits_{m=1}^{M}\beta_m b(x;\gamma_m)
\end{equation}
where $b(x;\gamma)$ are the basis functions while $\beta_m$ are the coefficients of the expansion.
Boosted models are fitted by minimising a loss function $L$ over the training data
\begin{equation}
    \min_{\beta_m, \gamma_m}\sum\limits_{i=1}^{N}L\left(y_i, \sum\limits_{m=1}^M \beta_m b(x_i;\gamma_m)\right)
\end{equation}
However, such problem is highly intensive in terms of computation. Therefore, what is done in the literature is approximating its solution by iteratively adding new basis functions to the current expansion. That is, we construct $f_m$ by solving for the optimal basis function and coefficients to add to $f_{m-1}$. Considering the square loss, we would have
\begin{equation}
    L(y_i, f_{m-1}(x_i)+\beta_m b(x_i;\gamma))=(e_{im}-\beta_m b(x_i;\gamma))^2
\end{equation}

\subsection{Boosted trees}
Combining several trees $T(\cdot, \Theta_m)$, we obtain the boosted tree model
\begin{equation}
    f_M(x)=\sum\limits_{m=1}^M T(x;\Theta_m)
\end{equation}
Thus, at each step of the iterative optimisation procedure, we have to solve
\begin{equation}\label{boosting_minimisation_problem}
    \hat{\Theta}_m=\argmin{\Theta_m}\sum\limits_{i=1}^{n}L(y_i, f_{m-1}(x_i)+T(x_i;\Theta_m))
\end{equation}
Remember, $\Theta_m$ refers to the parameters of the $m^{th}$ tree, $\Theta_m=\{R_{jm}, \gamma_{jm}\}_1^{J_m}$
\subsection{Gradient boosting}
In order to robustly solve Equation \ref{boosting_minimisation_problem}, gradient boosting considers the following problem
\begin{equation}
    \hat{\Theta}_m=\argmin{\Theta_m}\sum\limits_{i=1}^{N}\left(-g_{im}-T(x_i;\Theta_m)\right)^2
\end{equation}
where $g_{im}$ is the gradient of $L(f)=\sum\limits_{i=1}^n L(y_i, f(x_i))$ evaluated at $f_{m-1}$. Put simply, we are fitting the $m^{th}$ tree to the negative of the gradient values of $f$ through least squares.
In order to solve our quantile regression tasks through gradient boosting, all we need to do is specifying the pinball loss as our criterion to guide the minimisation algorithm. See Figure \ref{fig:gradient_boosting} for a visualisation.
\begin{figure}
    \includegraphics[width=\textwidth]{images/gradient_boosting.png}
    \caption{Gradient boosting regression}
    \label{fig:gradient_boosting}
    % image generated on coolab to be quick
\end{figure}



% \section{Kernel methods}
\section{Kernel quantile regression}
The idea of quantile regression has been extended to kernel methods by Takeuchi et al. \cite{takeuchi2006nonparametric}.
There, they minimise a risk functional plus regulariser defined as follows.
\begin{equation}\label{eq:kqr_min1}
    Risk[f]:=\frac{1}{m}\sum\limits_{i=1}^{m}\rho_q(y_i-f(x_i))+\frac{\tau}{2}\|w\|_\mathcal{H}^2
\end{equation}
where $f=w+b$, $w \in \mathcal{H}$ and $b \in \mathbb{R}$.
Using the link between RKHS and feature spaces, we can rewrite $f(x)=\langle w, \phi(x) \rangle_{\mathcal{H}}+b$. 
% Moreover, note that the RKHS norm is defined as follows $\|f\|_{\mathcal{H}}=\inf\{\|w\|_{\mathcal{F}}:w\in \mathcal{F},f(x)=\langle w,\varphi (x)\rangle _{\mathcal{F}},\forall x\in X\}$, where $\mathcal{F}$ is the feature space.
 Doing so we obtain a minimisation problem equivalent to minimising equation \ref{eq:kqr_min1}.
\begin{equation}\label{eq:kqr_min2}
    \begin{split}
        \min_{w,b} \quad & C \sum \limits_{i=1}^{m} \hbox{$q(y_i - \langle w, \phi(x_i) \rangle_{\mathcal{H}} - b)$} \\
        & + \hbox{$(1-q)(-y_i + \langle w, \phi(x_i) \rangle_{\mathcal{H}} + b)$} \\
        & + \frac{1}{2}\|w\|_{\mathcal{H}}^2
    \end{split}
    \end{equation}
Note the division by $\tau$ so that $C:=\frac{1}{\tau m}$.
\\
We can next rephrase the optimisation in Equation \ref{eq:kqr_min2} by introducing the slack variables $\xi_i$ and $\xi_i^*$.
\begin{equation}\label{eq:kqr_min3}
    \begin{aligned}
        \min_{w,b,\xi_i,\xi_i^*} \quad & C \sum \limits_{i=1}^{m}
        q \xi_i+ (1-q)\xi_i^*+ \frac{1}{2}\|w\|_{\mathcal{H}}^2\\
    \textrm{s.t.} \quad & y_i-\langle w, \phi(x_i) \rangle_{\mathcal{H}}-b \leq \xi_i\\
    & -y_i+\langle w, \phi(x_i) \rangle_{\mathcal{H}}+b \leq \xi_i^*\\
      &\xi_i\geq0    \\
      &\xi_i^*\geq0    \\
    \end{aligned}
    \end{equation}
In order to make it more compact, we rewrite Equation \ref{eq:kqr_min3} in matrix notation.
\begin{equation}\label{eq:kqr_min4}
    \begin{aligned}
        \min_{w,b\xi,\xi^*} \quad & C q \xi^\intercal \mathbb{1}+ C (1-q) (\xi^*)^\intercal \mathbb{1}+ \frac{1}{2}w^\intercal w\\
    \textrm{s.t.} \quad & y-\Phi^\intercal w -b \preceq \xi\\
    & -y+\Phi^\intercal w +b \preceq \xi^*\\
      &\xi\succeq0    \\
      &\xi^*\succeq0    \\
    \end{aligned}
    \end{equation}
Consider now, the Lagrangian $\mathcal{L}$ associated to Equation \ref{eq:kqr_min4}
\begin{equation}\label{eq:kqr_min5}
    \begin{split}
        L(w, b, \xi, \xi^*) = \quad & \hbox{$C q \xi^\intercal \mathbb{1} + C (1-q) (\xi^*)^\intercal \mathbb{1} + \frac{1}{2} w^\intercal w$} \\
        & \hbox{$- \lambda^\intercal (\xi - y + \Phi^\intercal w + b)$} \\
        & \hbox{$- (\lambda^*)^\intercal (\xi^* + y - \Phi^\intercal w - b)$} \\
        & \hbox{$- \nu^\intercal \xi - (\nu^*)^\intercal \xi^*$}
    \end{split}
\end{equation}
The next step is deriving its dual formulation, since it is easier and more efficient to solve. This because the dual problem has the useful property of being always convex.

\begin{definition}
    The dual function associated to the Lagrangian $\mathcal{L}(x,\lambda, \nu)$ is given by $g(\lambda, \nu)=\underset{x}\inf \ \mathcal{L}(x,\lambda, \nu)$
\end{definition}
where $\lambda$ is called the Lagrange multiplier of the optimisation problem. Such dual formulation has an useful property, which is \begin{equation}\label{weak_duality}
    g(\lambda, \nu)\leq p^*
\end{equation}
where $p^*$ is the optimal value of your optimisation problem. In other words $g(\lambda, \nu)$ is a lower bound for the optimal $p^*$.
Consider now a simple Lagrangian $$\mathcal{L}(x,\lambda, \nu)=f(x)+\sum\limits_{i=1}^n \lambda_i r_i(x) +\sum\limits_{i=1}^{n} \nu_i h_i(x)$$
where $r_i(x)$ are inequality constraints while $h_i(x)$ are equality constraints of the problem. Then it can be noted that, the lower bound on $p^*$ is non trivial only when the Lagrange multiplier $\lambda \succeq 0$.
% when lagrange multiplier is negative, we have that g > p
Therefore, the idea is that by maximising the dual function subject to the constraint $\lambda \succeq 0$, we can obtain an approximate or perfect solution to the primal problem.
\\
To explain why we may or may not be able to attain the best solution to the primal problem by maximising its dual, we have to introduce the concept of duality.
\\
We use $d^*$ to denote the optimal value of the Lagrange dual problem; we can think of it as the best lower bound on $p^*$. 
\\
The Inequality \ref{weak_duality} is called weak duality. The difference $p^*-d^*$ is the optimal duality gap. It is the gap between the optimal value of the primal problem and the best lower bound on it that can be obtained from the Lagrange dual function. Moreover, note that the optimal duality gap is always nonnegative.
We say that strong duality holds, when the optimal duality gap is zero. In other words, the Lagrange dual bound is tight.
\\
Constraint qualifications are conditions under which strong duality holds. One of the most popular is Slater's condition.
\begin{proposition}
    Slater's condition reads as
    \begin{equation}\label{slater_condition}
        \begin{aligned}
            \exists x \in \textrm{relint} \ D \ \textrm{s.t.} & \ r_i(x)<0, \quad i=1, \dots, m \\
            & h_i(x)=0
        \end{aligned}
    \end{equation}
    Where relint $D$ is the relative interior of $ D:=\underset{i=0}{\overset{m}{\cap}} \operatorname {dom} (r_{i})$
\end{proposition}
Slater's theorem naturally follows.
\begin{theorem}
    If Slater's condition holds and the problem is convex then strong duality holds.
\end{theorem}
We can now check that our optimisation problem possesses strong duality by checking Slater's condition.
\\
In our case we do not have any equality constraint, so we do not have to worry about the $h_i(x)=0$ term in Equation \ref{slater_condition}. All we have to check is the convexity of our problem and that there exists an $x$ such that $r_i(x)<0$.
For convexity, a sufficient condition is the positive definiteness of $Q$ in the quadratic programming problem 
\begin{equation}
    \begin{aligned}
        \min \quad & x^\intercal Q x+ c^\intercal x \\        
        s.t \quad& Ax\preceq b
    \end{aligned}
\end{equation}
% In our case, equation \ref{eq:kqr_min5}, we have $w^\intercal w$, thus $Q$ is just the identity matrix which satisfies the positive definiteness requirement. 
This condition is easily checked by the fact that kernel matrices are by definition positive semidefinite.
% $\|w\|_{\mathcal{H}}^2= w^\intercal w= \alpha^\intercal K \alpha$, the last element is positive semidefinitive which in turn implies also $w^\intercal w$ is also positive semidefinite.
Therefore, our problem is convex.
Next we check that Slater's condition holds. Considering first the two non negative constraints on $\xi$ and $\xi^*$, we conclude that $\xi$  and $\xi^*$ have to be greater or equal to zero for the existence of an $x$ satisfying Slater's condition. Thus, let us suppose that $0 \leq \xi \leq \mu$ and $0 \leq \xi^* \leq \mu$.
\\
Next, let us consider the other two inequalities and make the following ansatz.
\\
\begin{equation}
    \begin{aligned}
        w=& \Phi^\intercal(\Phi \Phi^\intercal)^{-1} (y-b)
    \end{aligned}
\end{equation}
We then have for any $\xi > 0$ and $\xi^* > 0$  that
\begin{equation}
    \begin{aligned}
        -\xi + y -\Phi\Phi^\intercal(\Phi \Phi^\intercal)^{-1}(y-b)-b<&0
        \\
        -\xi^* - y +\Phi\Phi^\intercal(\Phi \Phi^\intercal)^{-1}(y-b)+b<&0
    \end{aligned}
\end{equation}
Hence, we conclude that our problem satisfies Slater's condition. Therefore, the solution of the dual and primal problems are equivalent.
\\
We end this section with the derivation of the dual problem, that is, the convex problem, which we will solve in order to get the quantiles prediction of our quantile kernel algorithm.
\\
First, take the dual function from Equation \ref{eq:kqr_min5}.
\begin{equation}
    \begin{aligned}
        g(\lambda, \lambda^*, \nu, \nu^*)= & \inf_{\xi, \xi^*, w, b} \mathcal{L}(w,b,\xi,\xi^*,\lambda, \lambda^*, \nu, \nu^*)
\end{aligned}
\end{equation}
Setting its derivatives to zero
\begin{equation}\label{eq:lagrange_derivatives}
    \begin{cases}
        \frac{\partial \mathcal{L}}{\partial w}=0 \implies w=\Phi^\intercal(\lambda-\lambda^*)
        \\
        \frac{\partial \mathcal{L}}{\partial b}=0 \implies (\lambda-\lambda^*)^\intercal\mathbb{1}=0
        \\
        \frac{\partial \mathcal{L}}{\partial \xi}=0 \implies Cq \mathbb{1}-\lambda- \nu=0
        \\
        \frac{\partial \mathcal{L}}{\partial \xi^*}=0 \implies C(1-q)\mathbb{1} -\lambda^* -\nu^*=0
    \end{cases}
\end{equation}
As pointed out previously, the lower bound resulting from the dual formulation is non trivial only when the Lagrange multipliers $\lambda$ are $\succeq 0$. Looking at the last two equations of the system \ref{eq:lagrange_derivatives}, this implies the following two constraints $\lambda \in [0, Cq\mathbb{1}]$ and $\lambda^*\in [0, C(1-q)\mathbb{1}]$.
\\
Substitute the conditions for an optimum into \ref{eq:kqr_min5}, we obtain the dual formulation.
% nu and nu^* go away because they are multipliers accounting for equality constraint, which in the optimum have to be satisfied hence are zero => h_i \times \nu=0
\begin{equation}
    \begin{split}
        g(\lambda, \lambda^*) = \quad & \hbox{$\xi^\intercal(Cq\mathbb{1} - \lambda - \nu) + (\xi^*)^\intercal(C(1-q)\mathbb{1} - \lambda^* - \nu^*)$} \\
        & \hbox{$- (\lambda - \lambda^*)^\intercal \Phi \Phi^\intercal (\lambda - \lambda^*)$} \\
        & \hbox{$+ (\lambda - \lambda^*)^\intercal y - (\lambda - \lambda^*)^\intercal b + \frac{1}{2} (\lambda - \lambda^*)^\intercal \Phi \Phi^\intercal (\lambda - \lambda^*)$} \\
        \\
        g(\lambda, \lambda^*) = \quad & \hbox{$0 + 0 - \frac{1}{2} (\lambda - \lambda^*)^\intercal \Phi \Phi^\intercal (\lambda - \lambda^*)$} \\
        & \hbox{$+ (\lambda - \lambda^*)^\intercal y - 0$} \\
        \\
        g(\lambda, \lambda^*) = \quad & \hbox{$- \frac{1}{2} (\lambda - \lambda^*)^\intercal \Phi \Phi^\intercal (\lambda - \lambda^*)$} \\
        & \hbox{$+ (\lambda - \lambda^*)^\intercal y$}
    \end{split}
\end{equation}
Defining $\alpha=(\lambda-\lambda^*)$ and letting $K$ be the kernel matrix, we have that the dual optimisation problem reads as follows
\begin{equation}\label{eq:kqr_min6}
    \begin{aligned}
        \max_{\alpha} \quad & -\frac{1}{2}\alpha^\intercal K\alpha+\alpha^\intercal y\\
    \textrm{s.t.} \quad & 
    C(q-1)\mathbb{1}\preceq \alpha \preceq Cq\mathbb{1}\\
    &\alpha^\intercal\mathbb{1}=1
    \end{aligned}
    \end{equation}
Switching sign, we rephrase it as a minimisation problem, which is the common practice in convex optimisation.
\begin{equation}\label{eq:kqr_min7}
    \begin{aligned}
        \min_{\alpha} \quad & +\frac{1}{2}\alpha^\intercal K \alpha-\alpha^\intercal y\\
    \textrm{s.t.} \quad & 
    C(q-1)\mathbb{1}\preceq \alpha \preceq Cq\mathbb{1}\\
    &\alpha^\intercal\mathbb{1}=1
    \end{aligned}
    \end{equation}
The kernel quantile regression estimator is then given by
\begin{equation}
    f(x)=\sum\limits_{i=1}^{n} \alpha_i k(x_i, x)+b
\end{equation}
Since our optimisation problem possesses strong duality and it is differentiable in both the objective and the constraint, we have that it must satisfy the Karush Kuhn Tucker conditions (KKT), see Section 5.5.3 \cite{boyd2004convex}.
Thanks to the KKT conditions on the primal optimisation problem we have that $f(x_i)=y_i \ \mathrm{for} \ \alpha_i \not \in \{C(q-1), Cq\}$. 
To see this, we have to consider the KKT conditions.
% optimal lambda times optimal x
\begin{equation}
    \begin{aligned}
    \lambda r_i(x)=&0, \quad i=1,\dots,n,
    \\
    \nabla \mathcal{L}(x)=&0
\end{aligned}
\end{equation}
In our setting we have
\begin{equation}
    \begin{aligned}
        \lambda_i(\xi_i-y_i+r_i)=&0
        \\
        \lambda_i^*(\xi_i^*+y_i-r_i)=&0
        \\
        \nu_i \xi_i=&0
        \\
        \nu_i^* \xi_i^*=&0
        \\
        \nabla \mathcal{L} =&0
    \end{aligned}
\end{equation}
Using the gradient of the Lagrangian of Equation \ref{eq:lagrange_derivatives}, we end up with
\begin{equation}
    \begin{aligned}
        \lambda_i(\xi_i-y_i+r_i)=&0
        \\
        \lambda_i^*(\xi_i^*+y_i-r_i)=&0
        \\
        (Cq-\lambda_i) \xi_i=&0
        \\
        (C(1-q)-\lambda_i) \xi_i^*=&0
    \end{aligned}
\end{equation}

Now, let us break into cases

\begin{equation}
\begin{cases}
    \lambda_i=Cq, \ \lambda_i^*=0 & 
    \hbox{$\implies \lambda_i-\lambda_i^*=Cq, \xi_i\leq 0, \xi^*=0$}
    \\
    &
    \hbox{$\implies \xi_i-y_i+f_i+b=0$}
    \\
    \lambda_i=0, \ \lambda_i^*=C(1-q) & 
    \hbox{$\implies \lambda_i-\lambda_i^*=C(q-1), \xi_i= 0, \xi^*\leq 0$} 
    \\
    & \hbox{$ \implies \xi_i^*+y_i-f_i-b=0$}
    \\
    0\leq \lambda_i< Cq, \ 0\leq \lambda_i^*< C(1-q) & 
    \hbox{$\implies \xi_i=0,  \xi^*_i= 0$}
    \\
    & \hbox{$\implies -y_i+f_i+b=0, \ y_i-f_i-b=0$} \\
\end{cases}
\end{equation}

Therefore, in order to retrieve $b$ we simply have to choose an index $i$ such that  $\alpha_i \not \in \{C(q-1), Cq\}$
and let
\begin{equation}
    \begin{aligned}
    b=&y_i-\sum\limits_{i=1}^n \alpha_i k(x_i,x)
    \end{aligned}
\end{equation}


% In our implementatiton we take the point that minsimizes the distance between the boundary of domain of alpha, in this way we are guaranteed we do not take ith index of alpha_i for retrieving b, because it that case it would not work.
%contribution is implementation kernel quantile regression for pythos users since there exists only r version
\subsection{Weather quantiles}
In order to get acquainted with the inner workings of the presented methods, this section covers an application explaining practical details and comparing results.
\\
The dataset used is the Melbourne daily maximum temperatures \citeW{hdrcde}. It contains the daily maximum temperatures in Melbourne, Australia, from 1981-1990, excluding leap days, see Figure \ref{fig:melbourne_temperature_data}.
Due to the bimodality of the data, such dataset is commonly used to give a difficult quantile regression problem \cite{hyndman1996estimating}, thus why we chose it. The observed bimodality is that a hot day is likely to be followed by either an equally hot day or one much cooler.
\begin{figure}[!h]
    \includegraphics[width=\textwidth]{images/melbourne_temperature.png}
    \caption{Melbourne temperatures dataset}
    \label{fig:melbourne_temperature_data}
\end{figure}
Hereafter, the results of the four presented methods on the Melbourne dataset are reported, see Figure \ref{fig:melbourne_quantiles_comparison} for a visualisation. Notice, the kernel considered here is the Gaussian RBF. Hyperparameters have been tuned through cross validation, see Appendix \ref{appendix:cross_validation}.
\begin{figure}[!h]
    \begin{subfigure}[b]{0.5\linewidth}
      \centering
      \includegraphics[width=1.1\textwidth]{images/melbourne_linear_quantile_regression.png} 
      \caption{Linear quantile regression} 
      \label{fig:melbourne_linear_quantile_regression} 
      \vspace{4ex}
    \end{subfigure}%% 
    \begin{subfigure}[b]{0.5\linewidth}
      \centering
      \includegraphics[width=1.1\textwidth]{images/melbourne_gradient_boosting_quantile_regression.png} 
      \caption{Gradient boosting quantile regression} 
      \label{fig:melbourne_gradient_boosting_quantile_regression} 
      \vspace{4ex}
    \end{subfigure} 
    \begin{subfigure}[b]{0.5\linewidth}
      \centering
      \includegraphics[width=1.1\textwidth]{images/melbourne_quantile_forest.png} 
      \caption{Quantile forest} 
      \label{fig:melbourne_quantile_forest} 
    \end{subfigure}%%
    \begin{subfigure}[b]{0.5\linewidth}
      \centering
      \includegraphics[width=1.1\textwidth]{images/melbourne_gaussian_rbf_kernel_quantile_regression.png}
      \caption{Kernel quantile regression, Gaussian RBF} 
      \label{fig:melborune_kernel_quantile_regression} 
    \end{subfigure} 
    \caption{Quantile regressors comparison Melbourne weather data}
    \label{fig:melbourne_quantiles_comparison} 
  \end{figure}

% tables
\begin{table}[!h]
\caption{Pinball loss of Melbourne data}
\begin{tabular}{lllll}
    \toprule
     & Linear qr & Gbm qr & Quantile forest & KQR \\
    \midrule
    Pinball loss & 11.278895 & 10.317612 & 10.340842 & \textbf{10.031546} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[!h]
    \caption{Pinball loss quantile-wise of Melbourne data}
    \begin{tabular}{lllll}
    \toprule
    Quantiles & Linear qr & Gbm qr & Quantile forest & KQR \\
    \midrule
    0.100000 & 0.710644 & 0.549232 & 0.562888 & \textbf{0.540145} \\
    0.200000 & 1.155014 & 0.938561 & 0.946712 & \textbf{0.903193} \\
    0.300000 & 1.417805 & 1.212671 & 1.222407 & \textbf{1.173627} \\
    0.400000 & 1.540108 & 1.399925 & 1.409293 & \textbf{1.368022} \\
    0.500000 & 1.574957 & 1.517281 & 1.484589 & \textbf{1.456053} \\
    0.600000 & 1.525114 & 1.498608 & 1.495474 & \textbf{1.447470} \\
    0.700000 & 1.397918 & 1.372183 & 1.362173 & \textbf{1.331375} \\
    0.800000 & 1.170140 & 1.115077 & 1.123195 & \textbf{1.096649} \\
    0.900000 & 0.787195 & \textbf{0.714075} & 0.734112 & 0.715013 \\
    \bottomrule
    \end{tabular}
\end{table}
As already pointed out, the quantile regression with $q=0.5$ corresponds to the standard regression problem, hence we can compare the proposed methods also in terms of the mean absolute error.
\begin{table}[!h]
\caption{Mean absolute error of Melbourne data}
\begin{tabular}{lllll}
    \toprule
     & Linear qr & Gbm qr & Quantile forest & KQR \\
    \midrule
    MAE & 3.253882 & 3.134805 & 3.095041 & \textbf{3.024305} \\
    \bottomrule
    \end{tabular}
\end{table}  
From these tables, we can see that kernel quantile regression outperformes the simple quantile regressor as well as the more complex models like quantile forest and gradient boosting quantile regression for the Melbourne temperatures dataset. Not only kernel quantile regression was the best in terms of total pinball loss but also the best in terms of each quantile pinball loss and mean absolute error.
Comparison has been carried out on further datasets, yielding similar conclusions as the one of above, to know more see Appendix \ref{appendix:quantile_regressor_extensive_comparison}.

We conclude this chapter with reporting the same kind of Tables \ref{tab:kernel pinball comparison}, \ref{tab:kernel pinball comparison quantile-wise}, \ref{tab:kernel mae comparison} and Figure \ref{fig:kernel quantile regressors comparison} comparing various kernel functions.

% tables
\begin{table}[!h]
    \caption{Kernels comparison pinball loss for Melbourne data}
    \label{tab:kernel pinball comparison}
    \begin{center}
    \begin{tabular}{lll}
        \toprule
        & Kernel & Pinball loss
        \\
        \midrule
        & Absolute Laplacian &  10.136551 \\
        & Matern 0.5/Laplacian & 10.136551  \\
        & Matern 1.5 & 10.082668  \\
        & Matern 2.5 & 10.0494190    \\

        & Matern $\infty$/Gaussian RBF &  10.031546 \\

        & Chi squared & \textbf{10.018701}       \\
        
        & Linear & 10.463752    \\
        & Periodic  & 10.493333\\
        & Polynomial & 10.073935     \\
        & Sigmoid & 10.496149            \\
        % & Gauss RBFx Absolute Laplacian & 10.150826 \\
        & Cosine & 16.254383    \\
        \bottomrule
        \end{tabular}
    \end{center}
    \end{table}
    
\begin{table}[!h]
    \caption{Kernels comparison pinball loss quantile-wise for Melbourne data}
    \label{tab:kernel pinball comparison quantile-wise}
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lllllllllll}
    \toprule
    Kernel/Quantiles & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
    \midrule
    Absolute Laplacian & 0.545490 &
    0.913866 &
    1.189372 &
    1.386753 &
    1.477606 &
    1.468567 &
    1.348241 &
    1.101059 &
    \textbf{0.705598} &
    \\
    Matern 0.5/Laplacian & 0.545490 &
    0.913866 &
    1.189372 &
    1.386753 &
    1.477606 &
    1.468567 &
    1.348241 &
    1.101059 &
    \textbf{0.705598} &
\\
    Matern 1.5 & 0.542709 &
    0.904605 &
    1.184761 &
    1.382041 &
    1.468031 &
    1.459695 &
    1.338982 &
    1.096042 &
    0.705803 &
  \\  
    Matern 2.5 & 0.541228 &
    0.902478 &
    1.175042 &
    1.374289 &
    1.465790 &
    1.453434 &
    1.334173 &
    \textbf{1.093902} &
    0.709083 &
    \\
    Matern $\infty$/Gaussian RBF & \textbf{0.540145} &
    \textbf{0.903193} &
    1.173627 &
    1.368022 &
    1.456053 &
    1.447470 &
    1.331375 &
    1.096649 &
    0.715013 &    
    \\
    Chi squared & 0.540200 &
    0.904170 &
    \textbf{1.170647} &
    \textbf{1.360472} &
    \textbf{1.449636} &
    1.446009 &
    \textbf{1.333024} &
    1.096450 &
    0.718095 &
    \\
    Linear & 0.565663 &
    0.947889 &
    1.250009 &
    1.437863 &
    1.506723 &
    1.478379 &
    1.358865 &
    1.135702 &
    0.782660 &
    \\
    Periodic & 0.554232 &
    0.930133 &
    1.222068 &
    1.422040 &
    1.518273 &
    1.509894 &
    1.398269 &
    1.160021 &
    0.778402 &
    \\
    Polynomial & 0.542649 &
    0.908764 &
    1.183810 &
    1.370962 &
    1.452645 &
    \textbf{1.442749} &
    1.331137 &
    1.103014 &
    0.738205 &
    \\
    Sigmoid & 0.572588 &
    0.948625 &
    1.244226 &
    1.433785 &
    1.506071 &
    1.480981 &
    1.359103 &
    1.140366 &
    0.810406 &
    \\
    Cosine & 0.755479 &
    1.343973 &
    1.802603 &
    2.123425 &
    2.307123 &
    2.367123 &
    2.262603 &
    1.971644 &
    1.320411 \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}


\begin{table}[!h]
    \caption{Kernels comparison mean absolute error Melbourne data}
    \label{tab:kernel mae comparison}
    \begin{center}
    \begin{tabular}{lll}
        \toprule
         & Kernel & MAE \\
        \midrule
        & Absolute Laplacian &  3.061553 \\
        & Matern 0.5/Laplacian &  3.061553 \\
        & Matern 1.5 &  3.046474 \\
        & Matern 2.5 &  3.034608 \\
        & Matern $\infty$/Gauss RBF &  3.024305 \\
        & Chi squared &    3.027183    \\
        & Linear &     3.142352\\
        & Periodic  & 3.118420 \\
        & Polynomial &     \textbf{3.022743} \\
        & Sigmoid &      3.108107       \\
        & Cosine &    4.849589 \\
        \bottomrule
        \end{tabular}
    \end{center}
    \end{table}  


% kernels figure

\begin{figure}[!h]
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_a_laplacian_kernel_quantile_regression.png} 
        \caption{Absolute Laplacian} 
        \label{} 
        \vspace{4ex}
    \end{subfigure}%% 
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_matern_0.5_kernel_quantile_regression.png} 
        \caption{Matern 0.5/Laplacian} 
        \label{} 
        \vspace{4ex}
    \end{subfigure} 
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_matern_1.5_kernel_quantile_regression.png} 
        \caption{Matern 1.5} 
        \label{} 
        \vspace{4ex}
    \end{subfigure}%%
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_matern_2.5_kernel_quantile_regression.png}
        \caption{Matern 2.5} 
        \label{} 
        \vspace{4ex}
    \end{subfigure} 
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_gaussian_rbf_kernel_quantile_regression.png}
        \caption{Matern $\infty$/Gaussian RBF} 
        \label{} 
        \vspace{4ex}
    \end{subfigure} 
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_chi_squared_kernel_quantile_regression.png}
        \caption{Chi squared} 
        \label{} 
        \vspace{4ex}
    \end{subfigure} 
    \end{figure}
\begin{figure}[!h]\ContinuedFloat
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_linear_kernel_quantile_regression.png}
        \caption{Linear} 
        \label{} 
        \vspace{4ex}
    \end{subfigure} 
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_periodic_kernel_quantile_regression.png}
        \caption{Periodic} 
        \label{} 
        \vspace{4ex}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_polynomial_kernel_quantile_regression.png}
        \caption{Polynomial} 
        \label{} 
        \vspace{4ex}
    \end{subfigure} 
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_sigmoid_kernel_quantile_regression.png}
        \caption{Sigmoid} 
        \label{} 
        \vspace{4ex}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\linewidth}
        \centering
        \includegraphics[width=1.1\textwidth]{images/melbourne_cosine_kernel_quantile_regression.png}
        \caption{Cosine} 
        \label{} 
    \end{subfigure} 
    \caption{Kernel quantile regressors comparison for Melbourne data}
    \label{fig:kernel quantile regressors comparison} 
\end{figure}

% \section{Kernel density estimation}
% \input{input_files/kde.tex}

% \section{Ensemble methods}
% \input{input_files/ensemble.tex}

% \section{DMLP}
% %gluon nn
% % train a beta distribution to learn its parameters
% \section{DeepAR}


% \subsection{Kernel herding}
% Select the best point forecasting method and create a probabilistic forecast by modelling its model errors with kernel herding (do something like the residual bootstrap ensembles if it is meaningful and possible to implement).
